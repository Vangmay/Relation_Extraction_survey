# State of Relation Extraction using Large Language Models

## Overview

This repository contains a comprehensive survey of recent advancements in Relation Extraction (RE) using Large Language Models (LLMs). The paper explores cutting-edge techniques and methodologies that are transforming the field of information extraction.

## Key Contributions

The research provides an in-depth analysis of three primary approaches to relation extraction with LLMs:

1. **Prompt Design**

   - Explores techniques like Chain of Thought (CoT)
   - Demonstrates how carefully crafted prompts can improve model performance

2. **Alignment Techniques**

   - Addresses challenges in low-incidence tasks
   - Introduces innovative approaches like QA4RE and RAG4RE
   - Shows how reformulating relation extraction can unlock LLM capabilities

3. **Universal Information Extraction (UIE)**
   - Proposes a unified framework for information extraction tasks
   - Aims to break down silos between different information extraction approaches

## Key Findings

- Significant performance improvements across multiple benchmarks
- Successful techniques for addressing LLM limitations in relation extraction
- Promising directions for future research in information extraction

## Datasets Explored

- DocRED
- TACRED
- New York Times Annotated Corpus
- CoNLL04
- ACE 2005

## Future Research Directions

- Improved pre-training strategies
- Multilingual dataset integration
- Document-level relation extraction
- Knowledge base-aware models
- Unified information extraction frameworks

## Citation

If you use this work in your research, please cite the original paper:

```bibtex
@article{Sachan2023RelationExtraction,
  title={State of relation extraction using LLMs: A report},
  author={Vangmay Sachan and Yanfei Dong},
  year={2023}
}
```

# Acknowledgements

This research was conducted as part of the Odyssey 2023/2024 program at the National University of Singapore.

> Feel free to contact me for further information!
